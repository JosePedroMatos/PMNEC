{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a5e73f4",
   "metadata": {},
   "source": [
    "\n",
    "# Polynomial Regression (No Pipelines)\n",
    "\n",
    "This notebook demonstrates **polynomial regression** in scikit-learn **without** using `Pipeline`.\n",
    "We'll explicitly expand features with `PolynomialFeatures`, fit `LinearRegression`/`Ridge`, plot the fitted curves, and compare metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78e76e0",
   "metadata": {},
   "source": [
    "\n",
    "## Learning goals\n",
    "\n",
    "- Expand features with `PolynomialFeatures.fit_transform` manually.\n",
    "- Fit `LinearRegression` and `Ridge` on the expanded arrays.\n",
    "- Compare different polynomial degrees and discuss under/overfitting.\n",
    "- Evaluate with MSE and \\(R^2\\) on train/test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19392fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup: imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c8a04",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Create a curved dataset\n",
    "\n",
    "We'll synthesize data from a quadratic curve with noise:\n",
    "\\[ y = 0.7 x^2 - 2 x + 5 + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 3) \\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reproducibility\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "# Features (x) and target (y)\n",
    "X = rng.uniform(-4, 4, 120)              # x-values between -4 and 4\n",
    "noise = rng.normal(0, 3, size=120)       # Gaussian noise\n",
    "y = 0.7 * X**2 - 2 * X + 5 + noise       # Quadratic relationship + noise\n",
    "\n",
    "# scikit-learn expects 2D X\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "print(\"X shape:\", X.shape, \" y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf38928",
   "metadata": {},
   "source": [
    "\n",
    "### Quick look at the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d75c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y)\n",
    "plt.title(\"Synthetic data: curved with noise\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e4623",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Split the data into train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6af024",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "print(\"Train size:\", X_train.shape[0], \" Test size:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f736b9f",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Fit linear vs. polynomial models (no pipelines)\n",
    "\n",
    "We'll compare a plain linear model (degree=1) with polynomial degrees 2 and 5.\n",
    "We **manually** call `PolynomialFeatures.fit_transform(X_train)` and `.transform(X_test)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abbc725",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_and_score_degree(degree=2, use_ridge=False, alpha=1.0):\n",
    "    # Create polynomial feature transformer (no pipelines)\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    Xtr_poly = poly.fit_transform(X_train)\n",
    "    Xte_poly = poly.transform(X_test)\n",
    "\n",
    "    # Choose estimator\n",
    "    if use_ridge:\n",
    "        reg = Ridge(alpha=alpha, random_state=42)\n",
    "    else:\n",
    "        reg = LinearRegression()\n",
    "\n",
    "    reg.fit(Xtr_poly, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    yhat_tr = reg.predict(Xtr_poly)\n",
    "    yhat_te = reg.predict(Xte_poly)\n",
    "\n",
    "    # Metrics\n",
    "    mse_tr = mean_squared_error(y_train, yhat_tr)\n",
    "    mse_te = mean_squared_error(y_test, yhat_te)\n",
    "    r2_tr = r2_score(y_train, yhat_tr)\n",
    "    r2_te = r2_score(y_test, yhat_te)\n",
    "\n",
    "    return {\n",
    "        \"poly\": poly, \"reg\": reg,\n",
    "        \"mse_tr\": mse_tr, \"mse_te\": mse_te,\n",
    "        \"r2_tr\": r2_tr, \"r2_te\": r2_te\n",
    "    }\n",
    "\n",
    "results = {\n",
    "    \"Linear (deg=1)\": fit_and_score_degree(degree=1),\n",
    "    \"Poly deg=2\": fit_and_score_degree(degree=2),\n",
    "    \"Poly deg=5\": fit_and_score_degree(degree=5),\n",
    "}\n",
    "\n",
    "for name, res in results.items():\n",
    "    coef_shape = res[\"reg\"].coef_.shape\n",
    "    print(name)\n",
    "    print(\"  Coef dim:\", coef_shape)\n",
    "    print(f\"  MSE train={res['mse_tr']:.3f} | test={res['mse_te']:.3f}\")\n",
    "    print(f\"  R^2  train={res['r2_tr']:.3f} | test={res['r2_te']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba3c14",
   "metadata": {},
   "source": [
    "\n",
    "### Visualize fits\n",
    "\n",
    "We'll plot the training points and each model's predicted curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Smooth x-grid for drawing curves\n",
    "x_line = np.linspace(X.min(), X.max(), 400).reshape(-1, 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_train, y_train, label=\"train\")\n",
    "plt.scatter(X_test, y_test, label=\"test\")\n",
    "\n",
    "for name, res in results.items():\n",
    "    poly, reg = res[\"poly\"], res[\"reg\"]\n",
    "    x_line_poly = poly.transform(x_line)\n",
    "    y_line = reg.predict(x_line_poly)\n",
    "    plt.plot(x_line, y_line, label=name)\n",
    "\n",
    "plt.title(\"Polynomial Regression fits by degree (no pipelines)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d7e8f",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Overfitting and regularization (Ridge, no pipelines)\n",
    "\n",
    "High-degree polynomials may **overfit**: very low training error but poor test performance.\n",
    "We'll try `Ridge` (L2 regularization) with various `alpha` values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058644ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "deg = 10\n",
    "alpha_values = [0.0, 1.0, 10.0, 100.0]  # 0.0 ~ LinearRegression\n",
    "\n",
    "def fit_degree_with_alpha(deg, alpha):\n",
    "    poly = PolynomialFeatures(degree=deg, include_bias=False)\n",
    "    Xtr_poly = poly.fit_transform(X_train)\n",
    "    Xte_poly = poly.transform(X_test)\n",
    "\n",
    "    if alpha == 0.0:\n",
    "        reg = LinearRegression()\n",
    "    else:\n",
    "        reg = Ridge(alpha=alpha, random_state=42)\n",
    "\n",
    "    reg.fit(Xtr_poly, y_train)\n",
    "    tr_pred = reg.predict(Xtr_poly)\n",
    "    te_pred = reg.predict(Xte_poly)\n",
    "\n",
    "    return poly, reg, tr_pred, te_pred\n",
    "\n",
    "print(f\"Degree={deg} with different Ridge alpha values\")\n",
    "for alpha in alpha_values:\n",
    "    poly, reg, tr_pred, te_pred = fit_degree_with_alpha(deg, alpha)\n",
    "    print(f\"alpha={alpha:>5} | MSE train={mean_squared_error(y_train, tr_pred):.3f} | test={mean_squared_error(y_test, te_pred):.3f} | R^2 test={r2_score(y_test, te_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8bd0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visual: degree-10 curves with different alpha (no pipelines)\n",
    "plt.figure()\n",
    "plt.scatter(X_train, y_train, label=\"train\")\n",
    "plt.scatter(X_test, y_test, label=\"test\")\n",
    "\n",
    "x_line = np.linspace(X.min(), X.max(), 400).reshape(-1, 1)\n",
    "\n",
    "for alpha in [0.0, 10.0, 100.0]:\n",
    "    poly = PolynomialFeatures(degree=10, include_bias=False)\n",
    "    poly.fit(X_train)\n",
    "    x_line_poly = poly.transform(x_line)\n",
    "\n",
    "    if alpha == 0.0:\n",
    "        reg = LinearRegression()\n",
    "        name = \"deg=10 (no ridge)\"\n",
    "    else:\n",
    "        reg = Ridge(alpha=alpha, random_state=42)\n",
    "        name = f\"deg=10 + Ridge (alpha={alpha})\"\n",
    "\n",
    "    Xtr_poly = poly.transform(X_train)\n",
    "    reg.fit(Xtr_poly, y_train)\n",
    "    y_line = reg.predict(x_line_poly)\n",
    "    plt.plot(x_line, y_line, label=name)\n",
    "\n",
    "plt.title(\"Effect of Ridge regularization at high degree (no pipelines)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9751b4bb",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Inspect learned coefficients (degree 2)\n",
    "\n",
    "Let's print the learned coefficients for degree 2 to see how the model captures the quadratic shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd68b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Refit a clean degree-2 model to show coefficients\n",
    "poly2 = PolynomialFeatures(degree=2, include_bias=False)\n",
    "Xtr2 = poly2.fit_transform(X_train)\n",
    "reg2 = LinearRegression().fit(Xtr2, y_train)\n",
    "\n",
    "feature_names = [\"x\", \"x^2\"]\n",
    "for name, val in zip(feature_names, reg2.coef_.ravel()):\n",
    "    print(f\"{name:<4} -> {val: .4f}\")\n",
    "print(\"Intercept ->\", reg2.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e717f",
   "metadata": {},
   "source": [
    "\n",
    "## ✍️ Mini-exercises\n",
    "\n",
    "1. Change the polynomial **degree** in the comparison section (e.g., try 3 and 8). How do MSE and \\(R^2\\) move?\n",
    "2. Increase the **noise** scale in the data. What happens to the chosen degree's performance?\n",
    "3. Compare `LinearRegression` vs `Ridge` across degrees (e.g., 2–10). Which `alpha` generalizes best?\n",
    "4. Replace the synthetic data with a **real dataset** (e.g., one feature from a CSV) and repeat the analysis.\n",
    "5. Set `include_bias=True` in `PolynomialFeatures` and observe how the intercept changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb7c26",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "- Polynomial regression = linear regression on expanded features \\([x, x^2, x^3, \\ldots]\\)).\n",
    "- Here we **did not** use pipelines; we called `fit_transform`/`transform` explicitly.\n",
    "- Higher degree increases flexibility but can overfit; regularization (e.g., Ridge) helps.\n",
    "- Always evaluate on held-out data (MSE, \\(R^2\\)).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
